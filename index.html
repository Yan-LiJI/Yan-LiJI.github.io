<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" href="favicon.ico" />
<link rel="bookmark" href="favicon.ico" type="image/x-icon"　/>
<title>Yanli JI (姬艳丽)</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=aGbEdhEAAAAJ&hl=en">Google Scholar</a></div>
<div class="menu-item"><a href="https://github.com/Yan-LiJI/">GitHub</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Yanli JI (姬艳丽)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="https://yan-liji.github.io/yanli.github.io/"><img src="yl.jpeg" alt="alt text" width="131px" height="160px" /></a>&nbsp;</td>
<td align="left"><p><b>PhD, Professor </b> <br />
<b>Sun Yat-sen University (Shenzhen)</b> <br />
E-mail: <a href="mailto:jiyanli82@gmail.com">jiyanli82@gmail.com</a></p>
</td></tr></table>
<h2>About me</h2>
<p>Yanli Ji is a Professor at Sun Yat-sen University (Shenzhen). She obtained her Ph.D degree from the Department of Advanced Information Technology, Kyushu University, Japan in Sep. 2012. She was a visiting researcher at the University of Tokyo from Nov. 2018 to Jan. 2020. Her research interests focus on Human-Centered Robot topics, e.g. embodied intelligence, multi-modal analysis, semantic analysis, etc.</p>
<p>中山大学智能工程学院教授、博士生导师。博士毕业于日本九州大学，东京大学客员研究员。入选深圳市高层次人才孔雀计划，入选2023年度AI华人女性青年学者榜。发表中科院JCR二区以上SCI期刊及CCF A国际会议论文50余篇，申请三十余项发明专利。中国计算机学会高级会员(CCF Senior Member)，中国图象图形学会青年工作委员会执行委员、副秘书长，VALSE 委员会 SAC主席。参与组织包括ACMMM 2021，ACMMM Asia 2021，ACCV2022等国际会议5次，国内学术峰会视觉与学习青年学者研讨会（VALSE）及 ACM SIGAI CHINA symposium in TURC等15余次。28th Australasian Database Conference国际会议Best Paper Award；第二十届中国虚拟现实大会最佳论文提名奖。</p>
<h2>招收博士后/博士</h2>
  课题组具有优越的科研环境和硬件设备，宽敞的实验场地，欢迎申请博士后/博士等职位。
  <h2>招收研究生</h2>
<p> 研究生申请基本要求： 踏实努力；对具身智能和机器人有很高兴趣，愿意付出时间钻研。良好的数理基础，动手能力强，具有团队合作意识. <p>
<p> 本科生研究计划:  欢迎本科生报名参与实验室研究工作，可参与实验室组会讨论，可选择线上线下不同方式定期交流。 <p>
  
<h2>Research</h2>
<h3>Research interests</h3>
<ul>
<li><p>Embodied Intelligence, Robots (具身智能、机器人)</p>
</li>
<li><p>Multimodal Analysis （多模态智能分析）</p>
</li>
<li><p>Vision Language Model （视觉-语言模型）</p>
</li>
</ul>
<h3>Selected Publications</h3>
<ol>
<li><p>K Gedamu, Y JI*, Y Yang, J Shao, H T Shen. Self-supervised Sub-Action Parsing Network for Semi-supervised Action Quality Assessment, TIP, 2024.  (IF = 10.8)  </p>
</li>
  <li><p> X Liang, Y JI*, W-S Zheng, W Zuo, X Zhu. SV-Learner: Support-Vector Contrastive Learning for Robust Learning with Noisy Labels, TKDE, 2024.  (IF = 8.9) </p>
</li>
  <li><p> Z Lin, Y JI*, Y Yang, Independence Adversarial Learning for Cross-modal Sound Separation. AAAI, 2024. (CCF A)</p>
</li>
  <li><p> J Huang, Y JI*, Y Yang, H T Shen. Dominant SIngle-Modal SUpplementary Fusion (SIMSUF) For   Multimodal Sentiment Analysis, TMM, 2023.  (IF = 8.4) </p>
</li>
  <li><p>  K Gedamu, Y JI*, Y Yang, J Shao, H T Shen. Fine-grained Spatio-temporal Parsing Network for Action Quality Assessment, TIP, Vol.32, pp. 6386-6400, 2023. (IF = 10.8) </p>
</li>
  <li><p> Y JI, L Ye, H Huang, L Mao, Localization-assisted Uncertainty Score Disentanglement Network for Action Quality Assessment, ACM MM, pp. 8590–8597, 2023. (CCF A)</p>
</li>
  <li><p> J Huang, Y JI*, Y Yang, H T Shen. Cross-modality Representation Interactive Learning for Multimodal Sentiment Analysis. ACM MM, 2023. (CCF A)</p>
</li>
  <li><p> K Gedamu, Y JI*, L Gao, Y Yang, H T Shen. Relation-mining self-attention network for skeleton-based human action recognition, Pattern Recognition, Vol. 139, 109455, 2023.  (IF = 8.6) </p>
</li>
  <li><p> Y JI, S Ma, X Xu, X Li, HT Shen, Self-supervised Fine-grained Cycle-Separation Network (FSCN) for Visual-Audio Separation, TMM, Vol.25, pp. 5864-5876, 2022. (IF = 8.4) </p>
</li>
  <li><p> L Gao, Y JI*, Y Yang, H T Shen, Global-local Cross-view Fisher Discrimination for View-Invariant Action Recognition, ACM MM, pp. 5255–5264, 2022. (CCF A) </p>
</li>
  <li><p>  G Wang, X Xu, F Shen, H Lu, Y JI, HT Shen. Cross-modal Dynamic Networks for Video Moment Retrieval with Text Query, TMM, 2022. (IF = 8.4) </p>
</li>
  <li><p>Y JI, Y Hu, Y Yang, HT Shen. Region Attention Enhanced Unsupervised Cross-Domain Facial Emotion Recognition, IEEE Transactions on Knowledge and Data Engineering, Vol. 35(4), pp. 4190-4201, 2023. (IF = 8.9)  </p>
</li>
  <li><p> X Zhu, H Li, HT Shen, Z Zhang, Y JI, Y Fan. Fusing functional connectivity with network nodal information for sparse network pattern learning of functional brain networks, Information Fusion, Vol. 75, 131-139, 2021.  </p>
</li>
  <li><p>  S Ma, Y JI*, X Xu, X Zhu. Vision-guided Music Source Separation via a Fine-grained Cycle-Separation Network, ACMMM, 4202-4210, 2021. (CCF A) </p>
</li>
  <li><p> L Gao, Y JI*, X Xu, X Zhu, HT Shen. View-invariant Human Action Recognition via View Transformation Network, TMM, Vol. 24, pp. 4493-4503, 2022. (IF = 8.4) </p>
</li>
  <li><p> K Gedamu, Y JI*,Y Yang, L Gao, HT Shen. Arbitrary-view human action recognition via novel-view action generation, Pattern Recognition, Vol. 118, 108043, 2021.(IF = 8.6) </p>
</li>
  <li><p> Y Fu, M Zhang, X Xu, Z Cao, C Ma, Y JI, K Zuo, H Lu. Partial Feature Selection and Alignment for Multi-Source Domain Adaptation, CVPR, 2021. (CCF A) </p>
</li>
   <li><p> M Zhang, Y Yang, X Chen, Y JI, X Xu, J Li, HT Shen. Multi-stage aggregated transformer network for temporal language localization in videos, CVPR, 2021. (CCF A) </p>
</li>
   <li><p> L Peng, Y Yang, X Zhang, Y JI, H Lu, HT Shen. Answer Again: Improving VQA with Cascaded-Answering Model, IEEE Transactions on Knowledge and Data Engineering, 2020. (IF = 8.9) </p>
</li>
   <li><p> Y JI, Y Yang, F Shen, HT Shen, WS Zheng. Arbitrary-view Human Action Recognition: A Varying-view RGB-D Action Dataset, TCSVT, Vol. 31 (1), 289-300, 2020.(IF = 8.3) </p>
</li>
   <li><p>  J Wei, X Xu, Y Yang, Y JI, Z Wang, HT Shen. Universal Weighting Metric Learning for Cross-modal Matching, CVPR, 2020. (CCF A) </p>
</li>
   <li><p> Y Li, A Bozic, T Zhang, Y JI, T Harada, M Nießner. Learning to Optimize Non-Rigid Tracking, CVPR, 2020. (CCF A)  </p>
</li>
   <li><p> Y JI, Y Zhan, Y Yang, X Xu, F Shen, HT Shen, A Context Knowledge Map Guided Coarse-to-Fine Action Recognition, TIP, Vol. 29, 2742-2752, 2019. (IF = 10.8) </p>
</li>
   <li><p> Y JI, F Xu, Y Yang, N Xie, HT Shen, T Harada. Attention Transfer (ANT) Network for View-invariant Action Recognition, ACM MM, 574-582, 2019. (CCF A) </p>
</li>
   <li><p>Y JI, Y Yang, F Shen, HT Shen, X Li. A Survey of Human Action Analysis in HRI Applications, TCSVT, Vol. 30 (7), 2114-2128, 2020. (IF = 8.3) </p>
</li>
</ol>
<h3>Patents</h3>
<ol>
<li><p>  一种基于视觉头部检测的自动考勤方法, ZL201711161391.5, 2021-03.</p>
</li>  
  <li><p>  一种基于视线估计的注意力智能监督方法, ZL201710546644.4, 2020-04.</p>
</li>  
  <li><p>  一种基于深度信息和校正方式的手部姿态估计方法, ZL201610321710.3, 2019-08.</p>
</li>  
  <li><p>  一种基于深度数据的手部全局姿态检测方法, ZL201610093720.6, 2019-07. </p>
</li>  
  <li><p>  一种基于深度数据的三维手势姿态估计方法及系统, ZL201510670919.6, 2019-06. </p>
</li>  
  <li><p>  基于眼部关键点检测的 3D 视线方向估计方法, ZL201611018884.9, 2019-03-15.</p>
</li>  
  <li><p>  一种基于 STDW 的连续字符手势轨迹识别方法, ZL201610688950.7, 2019-01.</p>
</li>  
  <li><p>  一种基于眼动跟踪的人机交互方法, ZL201310684342.5, 2016-08.</p>
</li>  
  <li><p>  任意视角动作识别方法，ZL202011541269.2，2022-03.</p>
</li>  
  <li><p>  结合风格迁移的可控表情生成方法，ZL202011618332.8，2022-04.</p>
</li>  
   <li><p>  视觉辅助跨模态音频信号分离方法，ZL202011537001.1，2022-07. </p>
</li>  
   <li><p>  将文本转换为指定风格语音的方法，ZL202010128298.X，2023-04-18.</p>
</li>  
   <li><p>  基于质量分数解耦的动作质量评估方法，202310465335X，2023-04-26.</p>
</li>  
</ol>
  <h2>学术报告</h2>
<ol>
<li><p> 中国模式识别与计算机视觉大会（PRCV，CCF-C），报告题目：Human Centric Vision Understanding for Human Robot Interaction，2018.11。</p>
</li>
<li><p> 视觉与学习青年学者研讨会（VALSE），报告题目：自监督学习框架下的音视频理解研究，2021.10。</p>
</li>
<li><p> 中国图象图形学学会青年科学家会议，报告题目：以人为中心的音视频理解研究，2021.12。</p>
</li>
  <li><p> 中国图象图形大会，报告题目：多模态情感认知中的模态协同学习探讨，2024.05。</p>
</li>
</ol>
<h2>Datasets</h2>
<ol>
</li>  
  <li><p> FineFS database: https://github.com/./FineFS-dataset. </p>
</li>  
 <ul> 
</td>
</tr>
</table>
</body>
</html>
